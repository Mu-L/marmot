# Marmot v2.0 Configuration
# Leaderless SQLite Replication

# ==============================================================================
# NODE IDENTITY
# ==============================================================================

# Unique identifier for this node (0 = auto-generate from machine ID)
node_id = 0

# Directory for node data (logs, state, etc.)
# Place existing SQLite .db files here - they will be imported on first startup
data_dir = "./marmot-data"

# ==============================================================================
# TRANSACTION MANAGER
# ==============================================================================

[transaction]
# Transaction timeout without heartbeat (seconds)
heartbeat_timeout_seconds = 10

# Time window for Last-Write-Wins conflict resolution (seconds)
conflict_window_seconds = 10

# Lock wait timeout (seconds) - matches MySQL innodb_lock_wait_timeout
lock_wait_timeout_seconds = 50

# ==============================================================================
# CLUSTER MEMBERSHIP
# ==============================================================================

[cluster]
# Address to bind gRPC server (0.0.0.0 = all interfaces)
grpc_bind_address = "0.0.0.0"

# Address other nodes use to connect to this node
# Leave empty to auto-detect (hostname:grpc_port)
# Override for NAT/Docker: "public-ip:8080" or "service-name:8080"
grpc_advertise_address = ""

# gRPC port for cluster communication
grpc_port = 8080

# List of seed nodes to join cluster (empty = start as first node)
# Example: ["node1.example.com:8080", "node2.example.com:8080"]
seed_nodes = []

# Cluster authentication secret (PSK)
# All nodes in the cluster must use the same secret
# Can also be set via MARMOT_CLUSTER_SECRET environment variable (takes precedence)
# Leave empty to disable authentication (NOT recommended for production)
cluster_secret = ""

# Gossip protocol interval (milliseconds)
gossip_interval_ms = 1000

# Number of random peers to gossip with each round
gossip_fanout = 3

# Time before marking unresponsive node as suspect (milliseconds)
suspect_timeout_ms = 5000

# Time before declaring suspect node as dead (milliseconds)
dead_timeout_ms = 10000

# ==============================================================================
# REPLICATION
# ==============================================================================

[replication]
# Default write consistency level
# Options: ONE, TWO, THREE, QUORUM, ALL, LOCAL_ONE
default_write_consistency = "QUORUM"

# Default read consistency level
default_read_consistency = "LOCAL_ONE"

# Timeout for write operations (milliseconds)
write_timeout_ms = 5000

# Timeout for read operations (milliseconds)
read_timeout_ms = 2000

# Enable anti-entropy background sync
# Automatically catches up lagging nodes by replaying missed transactions
enable_anti_entropy = true

# Anti-entropy sync interval (seconds)
# How often to check for and repair replication lag (default: 60 = 1 minute)
anti_entropy_interval_seconds = 60

# Delta sync threshold (transactions)
# If a node lags by more than this many transactions, use snapshot instead of delta sync
delta_sync_threshold_transactions = 10000

# Delta sync threshold (seconds)
# If a node lags by more than this duration, use snapshot instead of delta sync
delta_sync_threshold_seconds = 3600

# Minimum GC retention (hours)
# Transaction logs kept for at least this long to support replication catch-up
# Must be >= delta_sync_threshold_seconds / 3600
gc_min_retention_hours = 2

# Maximum GC retention (hours)
# Force GC after this duration even if peers lagging (prevents unbounded growth)
gc_max_retention_hours = 24

# ==============================================================================
# CONNECTION POOL
# ==============================================================================

[connection_pool]
# Number of connections per pool
pool_size = 4

# Maximum idle time for connections (seconds)
max_idle_time_seconds = 10

# Maximum lifetime for connections (seconds)
max_lifetime_seconds = 300

# ==============================================================================
# METASTORE (PebbleDB) - CockroachDB-tested defaults
# ==============================================================================

[metastore]
# Block cache size in MB (default: 64)
cache_size_mb = 64

# MemTable size in MB (default: 64, CockroachDB-style)
memtable_size_mb = 64

# Number of MemTables (default: 2)
memtable_count = 2

# L0 compaction trigger threshold (default: 500, CockroachDB-style)
l0_compaction_threshold = 500

# L0 stop writes threshold (default: 1000, CockroachDB-style)
l0_stop_writes = 1000

# WAL sync settings for durability
wal_bytes_per_sync_kb = 512
wal_sync_interval_ms = 10

# ==============================================================================
# GRPC CLIENT
# ==============================================================================

[grpc_client]
# Send keepalive ping every N seconds
keepalive_time_seconds = 10

# Timeout for keepalive ping response (seconds)
keepalive_timeout_seconds = 3

# Maximum retry attempts for failed requests
max_retries = 3

# Backoff duration between retries (milliseconds)
retry_backoff_ms = 100

# ==============================================================================
# TRANSACTION COORDINATOR
# ==============================================================================

[coordinator]
# Timeout for 2PC prepare phase (milliseconds)
prepare_timeout_ms = 2000

# Timeout for 2PC commit phase (milliseconds)
commit_timeout_ms = 2000

# Timeout for 2PC abort phase (milliseconds)
abort_timeout_ms = 2000

# ==============================================================================
# DDL REPLICATION
# ==============================================================================

[ddl]
# DDL lock lease duration (seconds)
# Prevents concurrent DDL operations on the same database across the cluster
# If a node crashes while holding a DDL lock, the lock expires after this duration
lock_lease_seconds = 30

# Automatically rewrite DDL for idempotency
# Converts "CREATE TABLE foo" to "CREATE TABLE IF NOT EXISTS foo", etc.
# This allows safe replay of DDL operations after failures or during catch-up
enable_idempotent = true

# ==============================================================================
# MYSQL PROTOCOL SERVER
# ==============================================================================

[mysql]
# Enable MySQL wire protocol server
enabled = true

# Address to bind MySQL server (0.0.0.0 = all interfaces)
bind_address = "127.0.0.1"

# MySQL protocol port
port = 3307

# Maximum concurrent MySQL connections
max_connections = 1000

# Auto-increment ID generation mode
# "compact" (default) = 53-bit IDs safe for JavaScript (Number.MAX_SAFE_INTEGER)
# "extended" = 64-bit HLC-based IDs for maximum uniqueness
auto_id_mode = "compact"

# ==============================================================================
# REPLICA MODE (Read-Only Replicas)
# ==============================================================================

# [replica]
# # Enable read-only replica mode (mutually exclusive with cluster mode)
# enabled = false
#
# # Seed nodes to follow for replication (required when enabled)
# # Example: ["node1:8080", "node2:8080", "node3:8080"]
# follow_addresses = []
#
# # PSK for authenticating with cluster (required when enabled)
# # Can also be set via MARMOT_REPLICA_SECRET environment variable (takes precedence)
# secret = ""
#
# # Filter which databases to replicate (empty = all databases)
# # Supports exact names and glob patterns: ["myapp", "prod_*", "staging_*"]
# # System database (__marmot_system) is never replicated
# replicate_databases = []
#
# # Database discovery: poll for new databases (seconds)
# database_discovery_interval_seconds = 10
#
# # Cluster discovery: poll for cluster membership updates (seconds)
# discovery_interval_seconds = 30
#
# # Failover: max time to find alive node during failover (seconds)
# failover_timeout_seconds = 60
#
# # Reconnect: initial delay after disconnect (seconds)
# reconnect_interval_seconds = 5
#
# # Reconnect: maximum backoff delay (seconds)
# reconnect_max_backoff_seconds = 30
#
# # Initial sync: timeout for snapshot download (minutes)
# initial_sync_timeout_minutes = 30
#
# # Snapshot: number of parallel database downloads
# snapshot_concurrency = 3
#
# # Snapshot: cache TTL to avoid redundant snapshot creation (seconds)
# snapshot_cache_ttl_seconds = 30

# ==============================================================================
# SNAPSHOT (Not yet implemented)
# ==============================================================================

[snapshot]
# Enable periodic snapshots
enabled = false

# Snapshot interval (seconds)
interval_seconds = 300

# Snapshot storage type: peer, s3, webdav, sftp, local
store = "peer"

# Snapshot chunk size (MB)
chunk_size_mb = 5

# Number of parallel chunks for snapshot transfer
parallel_chunks = 5

# Number of changes before triggering full snapshot
incremental_threshold = 10000

# ==============================================================================
# LOGGING
# ==============================================================================

[logging]
# Enable verbose debug logging
verbose = false

# Log format: "console" (human-readable) or "json" (structured)
format = "console"

# ==============================================================================
# METRICS
# ==============================================================================

[prometheus]
# Enable Prometheus metrics endpoint
enabled = false

# Address to bind metrics server
address = "0.0.0.0"

# Prometheus metrics port
port = 9090
